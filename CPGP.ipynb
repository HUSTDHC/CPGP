{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.nn import Parameter\n",
    "from torch_geometric.nn.pool.topk_pool import topk,filter_adj\n",
    "from torch_geometric.nn import GCNConv, SAGEConv, GATConv, ChebConv, GraphConv,DataParallel\n",
    "\n",
    "from torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\n",
    "\n",
    "import argparse\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import copy as cp\n",
    "\n",
    "from torch.utils.data import random_split\n",
    "from torch_geometric.loader import DataLoader, DataListLoader\n",
    "\n",
    "import joblib\n",
    "from torch_geometric.data import Data\n",
    "import os\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "# torch.cuda.set_device(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GSAPool 模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GSAPool(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, pooling_ratio=0.5, alpha=0.6, pooling_conv=\"GCNConv\", fusion_conv=\"false\",\n",
    "                    min_score=None, multiplier=1, non_linearity=torch.tanh):\n",
    "        super(GSAPool,self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "\n",
    "        self.ratio = pooling_ratio\n",
    "        self.alpha = alpha\n",
    "\n",
    "        self.sbtl_layer = self.conv_selection(pooling_conv, in_channels)\n",
    "        self.fbtl_layer = nn.Linear(in_channels, 1)\n",
    "        self.fusion = self.conv_selection(fusion_conv, in_channels, conv_type=1)\n",
    "\n",
    "        self.min_score = min_score\n",
    "        self.multiplier = multiplier\n",
    "        self.fusion_flag = 0\n",
    "        if(fusion_conv!=\"false\"):\n",
    "            self.fusion_flag = 1\n",
    "        self.non_linearity = non_linearity\n",
    "\n",
    "    def conv_selection(self, conv, in_channels, conv_type=0):\n",
    "        if(conv_type == 0):\n",
    "            out_channels = 1\n",
    "        elif(conv_type == 1):\n",
    "            out_channels = in_channels\n",
    "        if(conv == \"GCNConv\"):\n",
    "            return GCNConv(in_channels,out_channels)\n",
    "        elif(conv == \"ChebConv\"):\n",
    "            return ChebConv(in_channels,out_channels,1)\n",
    "        elif(conv == \"SAGEConv\"):\n",
    "            return SAGEConv(in_channels,out_channels)\n",
    "        elif(conv == \"GATConv\"):\n",
    "            return GATConv(in_channels,out_channels, heads=1, concat=True)\n",
    "        elif(conv == \"GraphConv\"):\n",
    "            return GraphConv(in_channels,out_channels)\n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr=None, batch=None):\n",
    "        if batch is None:\n",
    "            batch = edge_index.new_zeros(x.size(0))\n",
    "        x = x.unsqueeze(-1) if x.dim() == 1 else x\n",
    "\n",
    "        #SBTL\n",
    "        score_s = self.sbtl_layer(x,edge_index).squeeze()\n",
    "        #FBTL\n",
    "        score_f = self.fbtl_layer(x).squeeze()\n",
    "        #hyperparametr alpha\n",
    "        score = score_s*self.alpha + score_f*(1-self.alpha)\n",
    "\n",
    "        score = score.unsqueeze(-1) if score.dim()==0 else score\n",
    "\n",
    "        if self.min_score is None:\n",
    "            score = self.non_linearity(score)\n",
    "        else:\n",
    "            score = softmax(score, batch)\n",
    "        perm = topk(score, self.ratio, batch)\n",
    "\n",
    "        #fusion\n",
    "        if(self.fusion_flag == 1):\n",
    "            x = self.fusion(x, edge_index)\n",
    "    \n",
    "        x = x[perm] * score[perm].view(-1, 1)\n",
    "        x = self.multiplier * x if self.multiplier != 1 else x\n",
    "        \n",
    "        batch = batch[perm]\n",
    "        edge_index, edge_attr = filter_adj(\n",
    "            edge_index, edge_attr, perm, num_nodes=score.size(0))\n",
    "\n",
    "        return x, edge_index, edge_attr, batch, perm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 评论网络特征提取模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GSAPoolModel(torch.nn.Module):\n",
    "    def __init__(self,args):\n",
    "        super(GSAPoolModel, self).__init__()\n",
    "\n",
    "        self.args = args\n",
    "        self.nhid = args.r_nhid\n",
    "\n",
    "        self.num_features = args.r_num_features\n",
    "        \n",
    "        self.alpha = args.r_alpha\n",
    "        self.pooling_ratio = args.r_pooling_ratio\n",
    "        self.dropout_ratio = args.dropout_ratio\n",
    "\n",
    "        self.pooling_layer_type = args.r_pooling_layer_type\n",
    "        self.feature_fusion_type = args.r_feature_fusion_type\n",
    "        self.out_feature_nhid = args.out_feature_nhid\n",
    "        \n",
    "        self.conv1 = self.conv_selection(args.conv,self.num_features, self.nhid)\n",
    "        self.pool1 = GSAPool(self.nhid, pooling_ratio=self.pooling_ratio, alpha = self.alpha, \n",
    "                     pooling_conv=self.pooling_layer_type, fusion_conv=self.feature_fusion_type)\n",
    "        \n",
    "        self.conv2 = self.conv_selection(args.conv,self.nhid, self.nhid)\n",
    "        self.pool2 = GSAPool(self.nhid, pooling_ratio=self.pooling_ratio, alpha = self.alpha, \n",
    "                     pooling_conv=self.pooling_layer_type, fusion_conv=self.feature_fusion_type)\n",
    "        \n",
    "        self.conv3 = self.conv_selection(args.conv,self.nhid, self.nhid)\n",
    "        self.pool3 = GSAPool(self.nhid, pooling_ratio=self.pooling_ratio, alpha = self.alpha, \n",
    "                     pooling_conv=self.pooling_layer_type, fusion_conv=self.feature_fusion_type)\n",
    "\n",
    "        self.lin1 = torch.nn.Linear(self.nhid*6, self.out_feature_nhid)\n",
    "      \n",
    "    def conv_selection(self, conv, in_channels,out_channels):\n",
    "        if conv == \"gcn\":\n",
    "            return GCNConv(in_channels, out_channels)\n",
    "        elif conv ==\"gat\":\n",
    "            return GATConv(in_channels, out_channels)\n",
    "        return None\n",
    "        \n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "\n",
    "\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x, edge_index, _, batch, _ = self.pool1(x, edge_index, None, batch)\n",
    "        x1 = torch.cat([gmp(x, batch), gap(x, batch)], dim=1)\n",
    "\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x, edge_index, _, batch, _ = self.pool2(x, edge_index, None, batch)\n",
    "        x2 = torch.cat([gmp(x, batch), gap(x, batch)], dim=1)\n",
    "\n",
    "        x = F.relu(self.conv3(x, edge_index))\n",
    "        x, edge_index, _, batch, _ = self.pool3(x, edge_index, None, batch)\n",
    "        x3 = torch.cat([gmp(x, batch), gap(x, batch)], dim=1)\n",
    "\n",
    "        x = torch.cat([x1, x2, x3], dim=1)\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = F.dropout(x, p=self.dropout_ratio, training=self.training)\n",
    "        # 返回隐藏层状态\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 传播网络特征提取模块(ASAPooling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn.pool import ASAPooling\n",
    "class ASAPoolModel(torch.nn.Module):\n",
    "    def __init__(self,args):\n",
    "        super(ASAPoolModel, self).__init__()\n",
    "\n",
    "        self.args = args\n",
    "        self.nhid = args.p_nhid\n",
    "\n",
    "        self.num_features = args.p_num_features\n",
    "        \n",
    "        self.pooling_ratio = args.p_pooling_ratio\n",
    "        self.dropout_ratio = args.dropout_ratio\n",
    "        self.out_feature_nhid = args.out_feature_nhid\n",
    "\n",
    "        self.conv1 = self.conv_selection(args.conv, self.num_features, self.nhid)\n",
    "        self.pool1 = ASAPooling(self.nhid, pooling_ratio=self.pooling_ratio, \n",
    "                     dropout=self.dropout_ratio)\n",
    "        \n",
    "        self.conv2 = self.conv_selection(args.conv, self.nhid, self.nhid)\n",
    "        self.pool2 = ASAPooling(self.nhid, pooling_ratio=self.pooling_ratio, \n",
    "                     dropout=self.dropout_ratio)\n",
    "        \n",
    "        self.conv3 = self.conv_selection(args.conv, self.nhid, self.nhid)\n",
    "        self.pool3 = ASAPooling(self.nhid, pooling_ratio=self.pooling_ratio, \n",
    "                     dropout=self.dropout_ratio)\n",
    "\n",
    "        self.lin1 = torch.nn.Linear(self.nhid*6, self.out_feature_nhid)\n",
    "        \n",
    "    def conv_selection(self, conv, in_channels,out_channels):\n",
    "        if conv == \"gcn\":\n",
    "            return GCNConv(in_channels, out_channels)\n",
    "        elif conv ==\"gat\":\n",
    "            return GATConv(in_channels, out_channels)\n",
    "        return None\n",
    "  \n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x, edge_index, _, batch, _ = self.pool1(x, edge_index, None, batch)\n",
    "        x1 = torch.cat([gmp(x, batch), gap(x, batch)], dim=1)\n",
    "\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x, edge_index, _, batch, _ = self.pool2(x, edge_index, None, batch)\n",
    "        x2 = torch.cat([gmp(x, batch), gap(x, batch)], dim=1)\n",
    "\n",
    "        x = F.relu(self.conv3(x, edge_index))\n",
    "        x, edge_index, _, batch, _ = self.pool3(x, edge_index, None, batch)\n",
    "        x3 = torch.cat([gmp(x, batch), gap(x, batch)], dim=1)\n",
    "\n",
    "        x = torch.cat([x1, x2, x3], dim=1)\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = F.dropout(x, p=self.dropout_ratio, training=self.training)\n",
    "        # 返回隐藏层状态\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 虚假新闻检测模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FakeNewsDetecModel(torch.nn.Module):\n",
    "    def __init__(self,args):\n",
    "        super(FakeNewsDetecModel, self).__init__()\n",
    "        self.relpy_layer = GSAPoolModel(args)\n",
    "        self.propagate_layer = ASAPoolModel(args)\n",
    "        self.r_nhid = args.r_nhid\n",
    "        self.p_nhid = args.p_nhid\n",
    "        self.num_classes = args.num_classes\n",
    "        self.lin1 = torch.nn.Linear(args.out_feature_nhid * 2, args.out_feature_nhid)\n",
    "        self.lin2 = torch.nn.Linear(args.out_feature_nhid, args.num_classes)\n",
    "    def forward(self, reply_nwtwork, propagate_network):\n",
    "        x1 = self.relpy_layer(reply_nwtwork)\n",
    "        x2 = self.propagate_layer(propagate_network)\n",
    "        \n",
    "        x = torch.cat([x1, x2], dim=1)\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = F.log_softmax(self.lin2(x), dim=-1)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 超参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('--seed', type=int, default=777, help='random seed')\n",
    "# cuda:id\n",
    "parser.add_argument('--device', type=str, default='cuda:1', help='specify cuda devices')\n",
    "parser.add_argument('--num_classes', type=int, default=2,\n",
    "                    help='num of classed')\n",
    "# hyper-parameters\n",
    "parser.add_argument('--dataset', type=str, default='gossipcop', help='[politifact, gossipcop]')\n",
    "parser.add_argument('--batch_size', type=int, default = 32, help='batch size')\n",
    "parser.add_argument('--lr', type=float, default=0.0001, help='learning rate')\n",
    "parser.add_argument('--weight_decay', type=float, default=0.0001, help='weight decay')\n",
    "parser.add_argument('--dropout_ratio', type=float, default=0.5, help='dropout ratio')\n",
    "parser.add_argument('--epochs', type=int, default=200, help='maximum number of epochs')\n",
    "parser.add_argument('--min_loss', type=float, default=1e10,help='min loss value')\n",
    "parser.add_argument('--patience', type=int, default=15,help='patience for earlystopping')\n",
    "parser.add_argument('--save_path', type=str, default='/home/dhc/workspace/GSAPool',help='path to save result')\n",
    "parser.add_argument('--training_times', type=int, default=20, help='')\n",
    "\n",
    "parser.add_argument('--out_feature_nhid', type=int, default=30, help='')\n",
    "# ASAPool超参数\n",
    "\n",
    "parser.add_argument('--p_nhid', type=int, default=25, help='hidden size')\n",
    "parser.add_argument('--p_concat', type=bool, default=True, help='node feature and graph embedding')\n",
    "parser.add_argument('--p_model', type=str, default='gcn', help='model type, [gcn, gat, sage]')\n",
    "parser.add_argument('--p_pooling_ratio', type=float, default=0.5,help='pooling ratio')\n",
    "parser.add_argument('--conv',type=str, default='gcn', help='model type, [gcn, gat, sage]')\n",
    "#GSAPool超参数\n",
    "parser.add_argument('--r_nhid', type=int, default=256, help='hidden size')\n",
    "parser.add_argument('--r_pooling_ratio', type=float, default=0.5,help='pooling ratio')\n",
    "parser.add_argument('--r_alpha', type=float, default=0.5,help='combination_ratio')\n",
    "parser.add_argument('--r_pooling_layer_type', type=str, default='GCNConv',help='GCNConv/SAGEConv/ChebConv/GATConv/GraphConv')\n",
    "parser.add_argument('--r_feature_fusion_type', type=str, default='GATConv',help='GATConv')\n",
    "\n",
    "args = parser.parse_known_args()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fake_news_net_dataset(reply_network_file, propagate_network_file):\n",
    "\n",
    "    #gossipcop  politifact\n",
    "    #gossipcop_reply_network__no_empty_text_with_tweet_top_5_edge_by_similar_include_tweet_reply_edge_60_85base\n",
    "    reply_network_data_file_path = \"/home/dhc/dataset/fake_news_net/processed/\" + reply_network_file\n",
    "    propagate_network_data_file_path = \"/home/dhc/dataset/fake_news_net/processed/\" + propagate_network_file\n",
    "    \n",
    "    reply_pkl = open(reply_network_data_file_path, 'rb')  ## 以二进制方式打开文件\n",
    "    propagate_pkl = open(propagate_network_data_file_path, 'rb')  ## 以二进制方式打开文件\n",
    "    \n",
    "    reply_network_data = joblib.load(reply_pkl)  ##用load()方法把文件内容序列化为Python对象\n",
    "    propagate_network_data = joblib.load(propagate_pkl)  ##用load()方法把文件内容序列化为Python对象\n",
    "    reply_pkl.close()\n",
    "    propagate_pkl.close()\n",
    "    dataset = []\n",
    "    nodes = 0\n",
    "    edges = 0\n",
    "    for item1, item2 in zip(reply_network_data, propagate_network_data):\n",
    "        nodes = nodes + len(item1.x)\n",
    "        edges = edges + len(item1.edge_index[0])\n",
    "        dataset.append([item1, item2])\n",
    "        #print(len(item1.edge_index))\n",
    "    print(nodes, edges)\n",
    "    return dataset\n",
    "\n",
    "def data_builder(args):\n",
    "    \n",
    "    dataset = load_fake_news_net_dataset(args.reply_network_file, args.propagate_network_file)\n",
    "    ratio = 0.8\n",
    "    # 训练集数目\n",
    "    num_training = int(len(dataset) * ratio)\n",
    "    # 测试集数目\n",
    "    num_test = len(dataset) - (num_training)\n",
    "    training_set,test_set = random_split(dataset,[num_training,num_test])\n",
    "    \n",
    "    train_loader = DataLoader(training_set, batch_size=args.batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_set,batch_size=args.batch_size,shuffle=False)\n",
    "    r_num_features = dataset[0][0].x.shape[1]\n",
    "    p_num_features = dataset[0][1].x.shape[1]\n",
    "\n",
    "    return train_loader, test_loader,r_num_features, p_num_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型准备工作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "361585 7243580\n"
     ]
    }
   ],
   "source": [
    "#training configuration    politifact_\n",
    "args.reply_network_file =\"politifact_reply_network.pkl\"\n",
    "args.propagate_network_file = \"politifact_propagate_network.pkl\"\n",
    "train_loader, test_loader,r_num_features,p_num_features = data_builder(args)\n",
    "\n",
    "args.r_num_features = r_num_features\n",
    "args.p_num_features = p_num_features\n",
    "\n",
    "model = FakeNewsDetecModel(args).to(args.device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FakeNewsDetecModel(\n",
      "  (relpy_layer): GSAPoolModel(\n",
      "    (conv1): GCNConv(768, 256)\n",
      "    (pool1): GSAPool(\n",
      "      (sbtl_layer): GCNConv(256, 1)\n",
      "      (fbtl_layer): Linear(in_features=256, out_features=1, bias=True)\n",
      "      (fusion): GATConv(256, 256, heads=1)\n",
      "    )\n",
      "    (conv2): GCNConv(256, 256)\n",
      "    (pool2): GSAPool(\n",
      "      (sbtl_layer): GCNConv(256, 1)\n",
      "      (fbtl_layer): Linear(in_features=256, out_features=1, bias=True)\n",
      "      (fusion): GATConv(256, 256, heads=1)\n",
      "    )\n",
      "    (conv3): GCNConv(256, 256)\n",
      "    (pool3): GSAPool(\n",
      "      (sbtl_layer): GCNConv(256, 1)\n",
      "      (fbtl_layer): Linear(in_features=256, out_features=1, bias=True)\n",
      "      (fusion): GATConv(256, 256, heads=1)\n",
      "    )\n",
      "    (lin1): Linear(in_features=1536, out_features=30, bias=True)\n",
      "  )\n",
      "  (propagate_layer): ASAPoolModel(\n",
      "    (conv1): GCNConv(10, 25)\n",
      "    (pool1): ASAPooling(25, ratio=0.5)\n",
      "    (conv2): GCNConv(25, 25)\n",
      "    (pool2): ASAPooling(25, ratio=0.5)\n",
      "    (conv3): GCNConv(25, 25)\n",
      "    (pool3): ASAPooling(25, ratio=0.5)\n",
      "    (lin1): Linear(in_features=150, out_features=30, bias=True)\n",
      "  )\n",
      "  (lin1): Linear(in_features=60, out_features=30, bias=True)\n",
      "  (lin2): Linear(in_features=30, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试函数\n",
    "def test(model,loader,test = False):\n",
    "    model.eval()\n",
    "    correct = 0.\n",
    "    loss = 0.\n",
    "    test_y = []\n",
    "    pre_y = []\n",
    "    for data in loader:\n",
    "        reply_network, propagate_network = data[0], data[1]\n",
    "        reply_network = reply_network.to(args.device)\n",
    "        propagate_network = propagate_network.to(args.device)\n",
    "        out = model(reply_network, propagate_network)\n",
    "        y = reply_network.y\n",
    "        \n",
    "        pred = out.max(dim=1)[1]\n",
    "        test_y.extend(y.cpu().numpy())\n",
    "        pre_y.extend(pred.cpu().numpy())\n",
    "        correct += pred.eq(y).sum().item()\n",
    "        loss += F.nll_loss(out,y,reduction='sum').item()\n",
    "    if test:\n",
    "        r = classification_report(test_y, pre_y)\n",
    "        C = confusion_matrix(test_y, pre_y)\n",
    "        print(\"confusion matrix: \", C, '\\n')\n",
    "        print(r, '\\n')\n",
    "    return correct / len(loader.dataset),loss / len(loader.dataset)\n",
    "        \n",
    "\n",
    "    #save result in txt\n",
    "def save_result(test_acc, save_path):\n",
    "    with open(os.path.join(save_path, 'result.txt'), 'a') as f:\n",
    "        test_acc *= 100\n",
    "        f.write(args.dataset+\";\")\n",
    "        f.write(\"pooling_layer_type:\"+args.r_pooling_layer_type+\";\")\n",
    "        f.write(\"feature_fusion_type:\"+args.r_feature_fusion_type+\";\")\n",
    "        f.write(str(test_acc))\n",
    "        f.write('\\r\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss:0.6740978726973901\taccuracy:0.6538461538461539\n",
      "Epoch0\n",
      "Model saved at epoch0\n",
      "Validation loss:0.6718902862988986\taccuracy:0.6538461538461539\n",
      "Epoch1\n",
      "Model saved at epoch1\n",
      "Validation loss:0.6672255396842957\taccuracy:0.6538461538461539\n",
      "Epoch2\n",
      "Model saved at epoch2\n",
      "Validation loss:0.6615683986590459\taccuracy:0.6538461538461539\n",
      "Epoch3\n",
      "Model saved at epoch3\n",
      "Validation loss:0.6463871231445899\taccuracy:0.6538461538461539\n",
      "Epoch4\n",
      "Model saved at epoch4\n",
      "Validation loss:0.6255476153813876\taccuracy:0.6730769230769231\n",
      "Epoch5\n",
      "Model saved at epoch5\n",
      "Validation loss:0.6313803562751183\taccuracy:0.6730769230769231\n",
      "Epoch6\n",
      "Validation loss:0.6199916692880484\taccuracy:0.6634615384615384\n",
      "Epoch7\n",
      "Model saved at epoch7\n",
      "Validation loss:0.6188980065859281\taccuracy:0.7019230769230769\n",
      "Epoch8\n",
      "Model saved at epoch8\n",
      "Validation loss:0.6120821237564087\taccuracy:0.6923076923076923\n",
      "Epoch9\n",
      "Model saved at epoch9\n",
      "Validation loss:0.5638550474093511\taccuracy:0.7307692307692307\n",
      "Epoch10\n",
      "Model saved at epoch10\n",
      "Validation loss:0.5909907359343308\taccuracy:0.7403846153846154\n",
      "Epoch11\n",
      "Validation loss:0.5800386300453773\taccuracy:0.7115384615384616\n",
      "Epoch12\n",
      "Validation loss:0.5479863927914546\taccuracy:0.7403846153846154\n",
      "Epoch13\n",
      "Model saved at epoch13\n",
      "Validation loss:0.5395508362696722\taccuracy:0.75\n",
      "Epoch14\n",
      "Model saved at epoch14\n",
      "Validation loss:0.5664750200051528\taccuracy:0.7019230769230769\n",
      "Epoch15\n",
      "Validation loss:0.6126467218765845\taccuracy:0.7019230769230769\n",
      "Epoch16\n",
      "Validation loss:0.5917607958500202\taccuracy:0.7019230769230769\n",
      "Epoch17\n",
      "Validation loss:0.567047866491171\taccuracy:0.7019230769230769\n",
      "Epoch18\n",
      "Validation loss:0.5178944560197684\taccuracy:0.7692307692307693\n",
      "Epoch19\n",
      "Model saved at epoch19\n",
      "Validation loss:0.5104576624356784\taccuracy:0.7596153846153846\n",
      "Epoch20\n",
      "Model saved at epoch20\n",
      "Validation loss:0.4937013112581693\taccuracy:0.7788461538461539\n",
      "Epoch21\n",
      "Model saved at epoch21\n",
      "Validation loss:0.5326237449279199\taccuracy:0.75\n",
      "Epoch22\n",
      "Validation loss:0.7061361395395719\taccuracy:0.5769230769230769\n",
      "Epoch23\n",
      "Validation loss:0.544244287105707\taccuracy:0.7307692307692307\n",
      "Epoch24\n",
      "Validation loss:0.49822023740181554\taccuracy:0.7980769230769231\n",
      "Epoch25\n",
      "Validation loss:0.5176096535645999\taccuracy:0.7019230769230769\n",
      "Epoch26\n",
      "Validation loss:0.46392120306308454\taccuracy:0.7788461538461539\n",
      "Epoch27\n",
      "Model saved at epoch27\n",
      "Validation loss:0.5009263157844543\taccuracy:0.7596153846153846\n",
      "Epoch28\n",
      "Validation loss:0.45696534560276914\taccuracy:0.7884615384615384\n",
      "Epoch29\n",
      "Model saved at epoch29\n",
      "Validation loss:0.4555428417829367\taccuracy:0.8076923076923077\n",
      "Epoch30\n",
      "Model saved at epoch30\n",
      "Validation loss:0.5255248409051162\taccuracy:0.7596153846153846\n",
      "Epoch31\n",
      "Validation loss:0.5493715955660894\taccuracy:0.7788461538461539\n",
      "Epoch32\n",
      "Validation loss:0.4824458360671997\taccuracy:0.7980769230769231\n",
      "Epoch33\n",
      "Validation loss:0.4661150872707367\taccuracy:0.7980769230769231\n",
      "Epoch34\n",
      "Validation loss:0.4602063252375676\taccuracy:0.7596153846153846\n",
      "Epoch35\n",
      "Validation loss:0.4561242484129392\taccuracy:0.7692307692307693\n",
      "Epoch36\n",
      "Validation loss:0.4960623887869028\taccuracy:0.7403846153846154\n",
      "Epoch37\n",
      "Validation loss:0.43290387667142427\taccuracy:0.7788461538461539\n",
      "Epoch38\n",
      "Model saved at epoch38\n",
      "Validation loss:0.43390725209162784\taccuracy:0.7692307692307693\n",
      "Epoch39\n",
      "Validation loss:0.434023937353721\taccuracy:0.7884615384615384\n",
      "Epoch40\n",
      "Validation loss:0.43195908344708955\taccuracy:0.7884615384615384\n",
      "Epoch41\n",
      "Model saved at epoch41\n",
      "Validation loss:0.4383156643464015\taccuracy:0.7788461538461539\n",
      "Epoch42\n",
      "Validation loss:0.5944016575813293\taccuracy:0.7403846153846154\n",
      "Epoch43\n",
      "Validation loss:0.42762691928790164\taccuracy:0.8365384615384616\n",
      "Epoch44\n",
      "Model saved at epoch44\n",
      "Validation loss:0.5420397795163668\taccuracy:0.7019230769230769\n",
      "Epoch45\n",
      "Validation loss:0.4897039097089034\taccuracy:0.75\n",
      "Epoch46\n",
      "Validation loss:0.46618478573285616\taccuracy:0.7788461538461539\n",
      "Epoch47\n",
      "Validation loss:0.4474710409457867\taccuracy:0.7692307692307693\n",
      "Epoch48\n",
      "Validation loss:0.4613124865752\taccuracy:0.7980769230769231\n",
      "Epoch49\n",
      "Validation loss:0.47932979693779576\taccuracy:0.7884615384615384\n",
      "Epoch50\n",
      "Validation loss:0.4793751056377704\taccuracy:0.7596153846153846\n",
      "Epoch51\n",
      "Validation loss:0.45819483353541446\taccuracy:0.7788461538461539\n",
      "Epoch52\n",
      "Validation loss:0.4820084250890292\taccuracy:0.7692307692307693\n",
      "Epoch53\n",
      "Validation loss:0.4639901335422809\taccuracy:0.7788461538461539\n",
      "Epoch54\n",
      "Validation loss:0.4709246021050673\taccuracy:0.8076923076923077\n",
      "Epoch55\n",
      "Validation loss:0.457800750549023\taccuracy:0.7788461538461539\n",
      "Epoch56\n",
      "Validation loss:0.43342394553698027\taccuracy:0.7788461538461539\n",
      "Epoch57\n",
      "Validation loss:0.45601707926163304\taccuracy:0.7788461538461539\n",
      "Epoch58\n",
      "Validation loss:0.4443964476768787\taccuracy:0.7692307692307693\n",
      "Epoch59\n",
      "Validation loss:0.4390542690570538\taccuracy:0.7788461538461539\n",
      "Epoch60\n"
     ]
    }
   ],
   "source": [
    "# 训练\n",
    "train_loss_his = []\n",
    "val_loss_his = []\n",
    "patience = 0\n",
    "min_loss = args.min_loss\n",
    "for epoch in range(80):\n",
    "    model.train()\n",
    "    tmp = []\n",
    "    for i, data in enumerate(train_loader):\n",
    "        reply_network, propagate_network = data[0], data[1]\n",
    "        reply_network = reply_network.to(args.device)\n",
    "        propagate_network = propagate_network.to(args.device)\n",
    "        out = model(reply_network, propagate_network)\n",
    "        y = reply_network.y\n",
    "        loss = F.nll_loss(out, y)\n",
    "        tmp.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    train_loss_his.append(sum(tmp)/len(tmp))\n",
    "    val_acc,val_loss = test(model,test_loader)\n",
    "    val_loss_his.append(val_loss)\n",
    "    print(\"Validation loss:{}\\taccuracy:{}\".format(val_loss,val_acc))\n",
    "    print(\"Epoch{}\".format(epoch))\n",
    "    if val_loss < min_loss:\n",
    "        torch.save(model.state_dict(),'latest.pth')\n",
    "        print(\"Model saved at epoch{}\".format(epoch))\n",
    "        min_loss = val_loss\n",
    "        patience = 0\n",
    "    else:\n",
    "        patience += 1\n",
    "    if patience > args.patience:\n",
    "        break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix:  [[30  6]\n",
      " [11 57]] \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.83      0.78        36\n",
      "           1       0.90      0.84      0.87        68\n",
      "\n",
      "    accuracy                           0.84       104\n",
      "   macro avg       0.82      0.84      0.82       104\n",
      "weighted avg       0.84      0.84      0.84       104\n",
      " \n",
      "\n",
      "Test accuarcy:0.8365384615384616\n"
     ]
    }
   ],
   "source": [
    "#test step\n",
    "model = FakeNewsDetecModel(args).to(args.device)\n",
    "model.load_state_dict(torch.load('latest.pth'))\n",
    "test_acc,test_loss = test(model,test_loader,True)\n",
    "print(\"Test accuarcy:{}\".format(test_acc))\n",
    "args.save_path = \"./\"\n",
    "save_result(test_acc, args.save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttrainLoss = train_loss_his"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.682618838090163,\n",
       " 0.6814675560364356,\n",
       " 0.67916667002898,\n",
       " 0.6783566658313458,\n",
       " 0.6686973571777344,\n",
       " 0.6706614723572364,\n",
       " 0.673806791122143,\n",
       " 0.660442943756397,\n",
       " 0.6525921317247244,\n",
       " 0.6434171933394212,\n",
       " 0.62601637840271,\n",
       " 0.641726920237908,\n",
       " 0.6282647206233098,\n",
       " 0.616279125213623,\n",
       " 0.5978762209415436,\n",
       " 0.5726625942266904,\n",
       " 0.6472890056096591,\n",
       " 0.6371900530961844,\n",
       " 0.5934585745518024,\n",
       " 0.5781888365745544,\n",
       " 0.5730788822357471,\n",
       " 0.5394675158537351,\n",
       " 0.5136245007698352,\n",
       " 0.5786612606965579,\n",
       " 0.5851367872494918,\n",
       " 0.5512387569134052,\n",
       " 0.5116601723891038,\n",
       " 0.4863892151759221,\n",
       " 0.4887762986696683,\n",
       " 0.47785725731116074,\n",
       " 0.44302507776480454,\n",
       " 0.7134305605521569,\n",
       " 0.5790034028200003,\n",
       " 0.5153266122707953,\n",
       " 0.48597310598079974,\n",
       " 0.4614940125208635,\n",
       " 0.4598308847500728,\n",
       " 0.41081584187654346,\n",
       " 0.39574238657951355,\n",
       " 0.4097527059224936,\n",
       " 0.411186369565817,\n",
       " 0.39067295766793764,\n",
       " 0.38245190794651324,\n",
       " 0.4072675234996356,\n",
       " 0.4464028959090893,\n",
       " 0.4495219634129451,\n",
       " 0.40731653800377476,\n",
       " 0.3691475276763623,\n",
       " 0.3618919448210643,\n",
       " 0.37765192412413084,\n",
       " 0.33498473580066973,\n",
       " 0.34244183737498063,\n",
       " 0.2991120173380925,\n",
       " 0.29662574369173783,\n",
       " 0.34042280797774976,\n",
       " 0.3235282233128181,\n",
       " 0.30933475723633397,\n",
       " 0.32889502896712375,\n",
       " 0.3145352051808284,\n",
       " 0.31522106321958393,\n",
       " 0.29383208545354694]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ttrainLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6740978726973901,\n",
       " 0.6718902862988986,\n",
       " 0.6672255396842957,\n",
       " 0.6615683986590459,\n",
       " 0.6463871231445899,\n",
       " 0.6255476153813876,\n",
       " 0.6313803562751183,\n",
       " 0.6199916692880484,\n",
       " 0.6188980065859281,\n",
       " 0.6120821237564087,\n",
       " 0.5638550474093511,\n",
       " 0.5909907359343308,\n",
       " 0.5800386300453773,\n",
       " 0.5479863927914546,\n",
       " 0.5395508362696722,\n",
       " 0.5664750200051528,\n",
       " 0.6126467218765845,\n",
       " 0.5917607958500202,\n",
       " 0.567047866491171,\n",
       " 0.5178944560197684,\n",
       " 0.5104576624356784,\n",
       " 0.4937013112581693,\n",
       " 0.5326237449279199,\n",
       " 0.7061361395395719,\n",
       " 0.544244287105707,\n",
       " 0.49822023740181554,\n",
       " 0.5176096535645999,\n",
       " 0.46392120306308454,\n",
       " 0.5009263157844543,\n",
       " 0.45696534560276914,\n",
       " 0.4555428417829367,\n",
       " 0.5255248409051162,\n",
       " 0.5493715955660894,\n",
       " 0.4824458360671997,\n",
       " 0.4661150872707367,\n",
       " 0.4602063252375676,\n",
       " 0.4561242484129392,\n",
       " 0.4960623887869028,\n",
       " 0.43290387667142427,\n",
       " 0.43390725209162784,\n",
       " 0.434023937353721,\n",
       " 0.43195908344708955,\n",
       " 0.4383156643464015,\n",
       " 0.5944016575813293,\n",
       " 0.42762691928790164,\n",
       " 0.5420397795163668,\n",
       " 0.4897039097089034,\n",
       " 0.46618478573285616,\n",
       " 0.4474710409457867,\n",
       " 0.4613124865752,\n",
       " 0.47932979693779576,\n",
       " 0.4793751056377704,\n",
       " 0.45819483353541446,\n",
       " 0.4820084250890292,\n",
       " 0.4639901335422809,\n",
       " 0.4709246021050673,\n",
       " 0.457800750549023,\n",
       " 0.43342394553698027,\n",
       " 0.45601707926163304,\n",
       " 0.4443964476768787,\n",
       " 0.4390542690570538]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_loss_his"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37_10.2",
   "language": "python",
   "name": "py37_10.2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
